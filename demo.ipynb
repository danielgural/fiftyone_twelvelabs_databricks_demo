{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne + Twelve Labs   + Mosaic AI Vector Search with Databricks  <img src=\"assets/voxel51_logo.png\" alt=\"Image2\" width=\"40\"/><img src=\"assets/twelve_labs_logo.jpg\" alt=\"Image2\" width=\"40\"/><img src=\"assets/db_logo.png\" alt=\"Image1\" width=\"80\"/>\n",
    "This notebook demonstrates how to build a complete visual search workflow using **FiftyOne**, **Twelve Labs Embeddings** and **Mosaic AI Vector Search on Databricks**.\n",
    "\n",
    "You will learn how to:\n",
    "- Set up your Databricks catalog and vector search endpoint\n",
    "- Load and index embeddings using Twelve Labs + FiftyOne\n",
    "- Query by image and text\n",
    "- Visualize results in the FiftyOne App\n",
    "\n",
    "üß† This integration helps you scale visual search over large datasets with a cloud-native vector database.\n",
    "\n",
    "üëâ For more, see the official [FiftyOne + Mosaic AI docs](https://docs.voxel51.com/integrations/mosaic.html) or the [Twelve Labs FiftyOne Plugin](https://github.com/danielgural/semantic_video_search)\n",
    "\n",
    "\n",
    "<img src=\"assets/fo_tl_db_diagram.png\" alt=\"Image2\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install fiftyone databricks-vectorsearch  python-dotenv  umap-learn twelvelabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/danielgural/semantic_video_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Set Up Environment Variables\n",
    "You need access to a Databricks account with **Vector Search enabled**.\n",
    "Follow these steps:\n",
    "1. **Create a Catalog**: Go to `Catalog` ‚Üí `Add Data` ‚Üí `Create a new Catalog`\n",
    "2. **Create a Vector Search Endpoint**: Go to `Compute` ‚Üí `Vector Search` ‚Üí `Create`\n",
    "3. **Create a Schema** inside your Catalog (FiftyOne will handle the columns later)\n",
    "\n",
    "‚ö†Ô∏è You must have a Personal Access Token for authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Option 1: Use a `.env` File\n",
    "You can store your credentials securely in a `.env` file:\n",
    "```bash\n",
    "FIFTYONE_BRAIN_SIMILARITY_MOSAIC_WORKSPACE_URL=https://your.cloud.databricks.com/\n",
    "FIFTYONE_BRAIN_SIMILARITY_MOSAIC_PERSONAL_ACCESS_TOKEN=your_token\n",
    "FIFTYONE_BRAIN_SIMILARITY_MOSAIC_CATALOG_NAME=your_catalog\n",
    "FIFTYONE_BRAIN_SIMILARITY_MOSAIC_SCHEMA_NAME=your_schema\n",
    "FIFTYONE_BRAIN_SIMILARITY_MOSAIC_ENDPOINT_NAME=your_endpoint\n",
    "TL_API_KEY=your_Twelve_Labs_API_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Option 2: Set Environment Variables in Code\n",
    "This is useful for notebooks or ephemeral environments:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_WORKSPACE_URL\"] = \"https://your.cloud.databricks.com/\"\n",
    "os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_PERSONAL_ACCESS_TOKEN\"] = \"your_token\"\n",
    "os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_CATALOG_NAME\"] = \"your_catalog\"\n",
    "os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_SCHEMA_NAME\"] = \"your_schema\"\n",
    "os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_ENDPOINT_NAME\"] = \"your_endpoint\"\n",
    "os.environ[\"DATABRICKS_HOST\"] = os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_WORKSPACE_URL\"]\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_PERSONAL_ACCESS_TOKEN\"]\n",
    "os.environ[\"TL_API_KEY\"]=your_Twelve_Labs_API_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_WORKSPACE_URL\"] = \"https://your.cloud.databricks.com/\"\n",
    "# os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_PERSONAL_ACCESS_TOKEN\"] = \"your_token\"\n",
    "# os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_CATALOG_NAME\"] = \"your_catalog\"\n",
    "# os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_SCHEMA_NAME\"] = \"your_schema\"\n",
    "# os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_ENDPOINT_NAME\"] = \"your_endpoint\"\n",
    "\n",
    "# These are critical for the SDK/MLflow auth\n",
    "# os.environ[\"DATABRICKS_HOST\"] = os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_WORKSPACE_URL\"]\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = os.environ[\"FIFTYONE_BRAIN_SIMILARITY_MOSAIC_PERSONAL_ACCESS_TOKEN\"]\n",
    "\n",
    "# For Twelve Labs\n",
    "# os.environ[\"TL_API_KEY\"]=your_Twelve_Labs_API_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëå Validate Authentication to Databricks + Twelve Labs\n",
    "Make sure your token works by initializing a Databricks and Twelve Labs SDK client and confirming your identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to check everything is in place:\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "client = WorkspaceClient()\n",
    "me = client.current_user.me()\n",
    "print(\"Authenticated as:\", me.user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twelvelabs import TwelveLabs\n",
    "import os\n",
    "\n",
    "tl_client = TwelveLabs(api_key=os.environ[\"TL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÅ Create an endpoint in your catalog.\n",
    "\n",
    "Be sure you have the following setup. Catalog -> Vector Search -> Endpoint. The FiftyOne integration will manage the rest. No worries about adding variables to your schema or settng up a vector search index, FiftyOne will manage it by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "\n",
    "# The following line automatically generates a PAT Token for authentication\n",
    "client = VectorSearchClient()\n",
    "\n",
    "# The following line uses the service principal token for authentication\n",
    "# client = VectorSearchClient(service_principal_client_id=<CLIENT_ID>,service_principal_client_secret=<CLIENT_SECRET>)\n",
    "\n",
    "\n",
    "client.create_endpoint(\n",
    "    name=\"vector_search_fiftyone_twelve_labs_cluster\",\n",
    "    endpoint_type=\"STANDARD\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until this endpoint is ready, any action before that can create a 500 or 400 HTTP Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Load the Quickstart-Video Dataset and Launch FiftyOne\n",
    "We will use the `quickstart-video` dataset from FiftyOne's built-in zoo to demonstrate embedding and vector indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart-video\")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![quickstart-video](./assets/qsv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings with Twelve Labs <img src=\"assets/twelve_labs_logo.jpg\" alt=\"Image2\" width=\"40\"/>\n",
    "\n",
    "Our next step is to add [Twelve Labs](https://www.twelvelabs.io/) embeddings to our video dataset. Twelve Labs provides state of the art embeddings, especially on videos that can be leveraged for powerful visualization and similarity worklows. \n",
    "\n",
    "Using our [plugin](https://github.com/danielgural/semantic_video_search) that we installed at the beginning, we can use our `create_twelve_labs_embeddings` operator to add these embeddings to our video dataset. To start, hit the \" ` \" button on your keyboard to open the operator list and type in  \"create_twelve_labs_embeddings\" and select the operator. Next, choose audio, visual, or both embeddings to generate on your video. You can also choose to compute on your whole dataset, selected samples, or your current view. \n",
    "\n",
    "If your dataset is large, you should check out [delegated operation](https://docs.voxel51.com/plugins/using_plugins.html#delegated-operations) for the plugin to avoid timing out the operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tl_plugin](./assets/tl_plugin.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate our Twelve Lab embeddings, Twelve Labs returns our embeddings in segments of clips. We store each of these embedding segments on our FiftyOne dataset as temporal detections to keep track of where each clip starts and end. \n",
    "\n",
    "Why do we need to do this? Well videos could be very long and when we do our search, we want to make sure we return the right **clip** not just the whole video! To start our search workflow, let's transform our dataset into a clip dataset using FiftyOne [`to_clips`](https://docs.voxel51.com/api/fiftyone.core.dataset.html?highlight=to_clips#fiftyone.core.dataset.Dataset.to_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.video as fouv\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "def create_clip_dataset(\n",
    "    dataset: fo.Dataset,\n",
    "    clip_field: str,\n",
    "    new_dataset_name: str = \"clips\",\n",
    "    overwrite: bool = True,\n",
    "    viz: bool = False,\n",
    "    sim: bool = False,\n",
    ") -> fo.Dataset:\n",
    "    clips = []\n",
    "    clip_view = dataset.to_clips(clip_field)\n",
    "    clip_dataset = fo.Dataset(name=new_dataset_name,overwrite=overwrite)\n",
    "    i = 0\n",
    "    last_file = \"\"\n",
    "    samples = []\n",
    "    for clip in clip_view:\n",
    "\n",
    "        out_path = clip.filepath.split(\".\")[0] + f\"_{i}.mp4\"\n",
    "        fpath = clip.filepath \n",
    "        fouv.extract_clip(fpath, output_path=out_path, support=clip.support)\n",
    "        clip.filepath = out_path\n",
    "        samples.append(clip)\n",
    "        clip.filepath = fpath\n",
    "        if clip.filepath == last_file:\n",
    "            i += 1\n",
    "        else:\n",
    "            i = 0\n",
    "        last_file = clip.filepath\n",
    "    clip_dataset.add_samples(samples)\n",
    "    clip_dataset.add_sample_field(\"Twelve Labs Marengo-retrieval-27 Embeddings\", fo.VectorField)\n",
    "    clip_dataset.set_field(\"Twelve Labs Marengo-retrieval-27 Embeddings\", clip_view.values(\"Twelve Labs Marengo-retrieval-27.embedding\"))\n",
    "    \n",
    "    return clip_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset = create_clip_dataset(dataset, \"Twelve Labs Marengo-retrieval-27\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this to grab embeddings \n",
    "clip_view = dataset.to_clips( \"Twelve Labs Marengo-retrieval-27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might have to run twice if DB is not ready, make sure to update brain key\n",
    "results = fob.compute_similarity(\n",
    "            clip_dataset,\n",
    "            brain_key=\"Twelve_Labs_Similarity\",\n",
    "            embeddings=clip_dataset.values(\"Twelve Labs Marengo-retrieval-27.embedding\"),\n",
    "            backend=\"mosaic\",\n",
    "            index_name=\"fiftyone_index\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by first image sample\n",
    "query =  clip_dataset.first().id\n",
    "view = clip_dataset.sort_by_similarity(query, brain_key=\"Twelve_Labs_Similarity\", k=3)\n",
    "session.view = view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img_sim](./assets/img_sim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search by text! In order to do so, we need to generate a text embedding from Twelve Labs and use that to search across our dataset. Let's try by searching for \"fast food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by text prompt\n",
    "query_txt = \"fast food\"\n",
    "\n",
    "res = tl_client.embed.create(\n",
    "  model_name=\"Marengo-retrieval-2.7\",\n",
    "  text=query_txt,\n",
    ")\n",
    "\n",
    "embedding = res.text_embedding.segments[0].embeddings_float\n",
    "view_txt = clip_dataset.sort_by_similarity(embedding, k=3, brain_key=\"Twelve_Labs_Similarity3\")\n",
    "session.view = view_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, our first video we can see Burger King!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![txt_sim](./assets/txt_sim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Video Embeddings\n",
    "\n",
    "We can also use our embeddings to visualize the distribution of our dataset! Using FiftyOne's `compute_visualization` we can generate our embedding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_visualization(\n",
    "            clip_dataset,\n",
    "            method=\"umap\", \n",
    "            brain_key=\"TwelveLabsVisualization\",\n",
    "            embeddings=clip_view.values(\"Twelve Labs Marengo-retrieval-27.embedding\")\n",
    "        )\n",
    "\n",
    "session.dataset = clip_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![emb_viz](./assets/embedding_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Mosaic index and run record\n",
    "mosaic_index.cleanup()\n",
    "dataset.delete_brain_run(\"mosaic_index\")\n",
    "#dataset.delete_brain_runs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSS310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
